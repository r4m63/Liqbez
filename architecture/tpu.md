# TPU: Специальная архитектура для AI

# Проблема: Ограничения классических процессоров в AI

Для современных задач искусственного интеллекта требуются огромные вычислительные мощности.
Традиционные процессоры (CPU) и графические ускорители (GPU) не всегда оптимальны для специфических задач машинного
обучения. Поэтому появились специализированные аппаратные решения, такие как Tensor
Processing Unit (TPU) — процессоры, разработанные Google для эффективного выполнения операций, связанных с AI.

# Ограничения существующих процессоров

## Ограничения CPU: почему он не подходит для AI

- CPU рассчитан на последовательные вычисления

  Когда процессор пытается работать с большими массивами данных, узкий канал памяти становится "пробкой".
  CPU имеет несколько мощных ядер, которые работают над разными задачами одновременно, но в пределах одной задачи CPU
  выполняет команды последовательно.
  AI же требует массового параллелизма

- Проблема "узкого места"

  CPU использует фон-неймановскую архитектуру, где данные хранятся в оперативной памяти и передаются через шину.
  Шина данных – узкое место: процессор ждёт загрузки данных вместо того, чтобы сразу выполнять вычисления.
  В AI-вычислениях CPU простаивает большую часть времени, ожидая данные, что сильно снижает эффективность.

- Слабая оптимизация для матричных операций

  Основные вычисления в нейросетях – это умножение матриц и векторов (тензорные операции).
  CPU не содержит специализированных блоков для матричных вычислений, выполняя их через общие арифметические модули, что
  делает процесс медленным.

## GPU как альтернатива: почему он тоже не идеальна?

Когда оказалось, что CPU не справляется с нейросетями, разработчики обратились к графическим процессорам (GPU). Они
оказались лучше из-за своей архитектуры, но тоже имели ограничения.

- GPU создан для графики, а не для AI

  Графический процессор содержит тысячи маломощных ядер, которые умеют работать параллельно.
  Это ускорило вычисления нейросетей, но GPU изначально разрабатывались для 3D-графики, а не для нейронных сетей.
  Они не были оптимизированы именно под тензорные вычисления.

- GPU по-прежнему ограничен памятью

  Хоть у GPU больше пропускная способность памяти, он всё равно зависит от передачи данных из оперативной памяти, что
  создаёт задержки.
  Массивные модели AI требуют огромных объёмов данных, а узкие места GPU в памяти становятся ограничивающим фактором.

- GPU слишком универсален

  Производители GPU (NVIDIA, AMD) создают универсальные решения: они должны работать не только с нейросетями, но и с
  рендерингом, физическими симуляциями и другими задачами.
  Универсальность = неоптимальность для конкретных вычислений AI.

Эти ограничения привели к созданию тензорных процессоров (TPU, Tensor Processing Units) — специализированных
чипов, разработанных Google, которые значительно ускоряют выполнение тензорных операций,
обеспечивая лучшую производительность и энергоэффективность по сравнению с GPU.

# Что такое TPU

TPU – это интегральная схема специального назначения, разработанная для выполнения конкретных задач в AI вычислениях.
Это специализированный ускоритель, созданный Google для ускорения нейросетевых вычислений. TPU предназначен только для
AI-задач и построен на принципах максимальной параллельности и эффективности обработки тензорных операций.

## Принцип работы

Параллельная обработка матричных операций – вместо работы с отдельными числами, TPU обрабатывает целые тензоры.
Минимизация задержек – TPU хранит данные внутри себя, не загружая их каждый раз из оперативной памяти.
Оптимизированные вычислительные блоки – TPU использует тензорные процессоры MXU (Matrix Multiply Unit), которые
специализированы для операций умножения матриц.
Минимизация общего назначения инструкций – TPU не содержит сложных управляющих механизмов, которые есть в CPU/GPU, что
делает его энергоэффективным.

### Главное отличие TPU от CPU/GPU:

CPU работает с отдельными числами (скалярами) и выполняет инструкции последовательно.
GPU работает с векторами (массивами чисел) и может параллельно выполнять одни и те же операции над разными данными.
TPU работает с матрицами (тензорами) и может обрабатывать сотни тысяч операций одновременно.

### Обработка тензорных операций.

Тензоры – это многомерные массивы данных (например, матрицы), которые используются в нейросетях. TPU разработан
специально для обработки тензоров и выполняет тысячи операций одновременно.

### Главный вычислительный блок TPU – MXU (Matrix Multiply Unit).

В TPU вместо обычных арифметико-логических устройств (ALU) используется тензорный процессор MXU.
MXU – это массив вычислительных ядер, которые выполняют массовые операции умножения матриц.
GPU тоже выполняет матричные операции, но у него нет выделенных тензорных блоков – всё делается через программные
библиотеки.

### Как TPU выполняет тензорные операции?

Обычно для умножения матриц CPU выполняет отдельные умножения и сложения в несколько шагов. В TPU матрицы загружаются
сразу в MXU, и все элементы умножаются одновременно.

CPU (медленно):

- Загружает число A из памяти
- Загружает число B
- Умножает A * B
- Записывает в память
- Повторяет всё снова

TPU (быстро):

- Загружает всю матрицу в MXU
- Все умножения выполняются сразу
- Записывает готовый результат без промежуточных шагов

### Основные отличия TPU:

- Специализированная архитектура для тензорных вычислений – без избыточных инструкций, характерных для CPU/GPU.
- Отказ от фон-неймановской модели – TPU хранит данные внутри самого процессора и не ждёт загрузки из оперативной
  памяти.
- Использование тензорных процессоров (MXU) – специализированные модули для умножения матриц работают быстрее, чем любые
  другие процессоры.
- Высокая параллельность – TPU может одновременно обрабатывать огромное количество данных, обеспечивая прирост
  производительности.

### Итог: почему TPU быстрее CPU и GPU?

| Фактор CPU GPU TPU             | CPU              | GPU            | TPU                  |
|--------------------------------|------------------|----------------|----------------------|
| **Использование памяти**       | Внешняя (RAM)    | Внешняя (VRAM) | Внутренняя           |
| **Основной тип вычислений**    | Последовательные | Параллельные   | Массово параллельные |
| **Оптимизация для матриц**     | Нет              | Частично       | Полностью            |
| **Выделенные тензорные блоки** | Нет              | Частично       | Да                   |
| **Быстродействие**             | Низкое           | Среднее        | Высокое              |

| Проблема                            | CPU | GPU | TPU |
|-------------------------------------|-----|-----|-----|
| **Последовательные вычисления**     | ✅   | ❌   | ❌   |
| **Зависимость от медленной памяти** | ✅   | ✅   | ❌   |
| **Отсутствие матричных блоков**     | ✅   | ✅   | ❌   |
| **Неоптимальная архитектура**       | ✅   | ✅   | ❌   |
| **Ограниченная параллельность**     | ✅   | ❌   | ❌   |             

## Как программируется TPU

TPU не программируется напрямую, как CPU или GPU. Вместо этого он управляется высокоуровневым ПО, таким как
**TensorFlow**.

**TensorFlow** — это открытая библиотека машинного обучения и искусственного интеллекта, разработанная Google.
Она предоставляет инструменты для создания, обучения и развертывания моделей AI. TensorFlow поддерживает различные
платформы, включая CPU, GPU и TPU. TensorFlow — это не единственный фреймворк, который поддерживает TPU, но он является
основным, так как TPU разработаны Google, и TensorFlow — их родной фреймворк.

**XLA** (Accelerated Linear Algebra) — это компилятор, встроенный в TensorFlow, который преобразует модель в машинный
код, оптимизированный для TPU.

Для TPU не пишут код на низкоуровневых языках (C, Assembly), потому что его инструкциями напрямую управляет фреймворк.

## Как работать с TPU

TPU используется как сопроцессор, работающий в паре с CPU.
TPU не может самостоятельно загружать модели или управлять файлами – этим занимается CPU.
TPU не имеет прямого доступа к данным на диске или в сети. CPU отвечает за загрузку, подготовку и передачу данных на
TPU. Это связано с архитектурой TPU, которая оптимизирована для выполнения вычислений, а не для управления данными.

- После компиляции через XLA на выходе получается бинарный код, который может выполняться на TPU.
- TensorFlow с CPU подключается к TPU и начинает загрузку модели в память, инициализирует её.
- Если доступно несколько ядер TPU, TensorFlow распределяет модель между ними.
- CPU переходит в режим управления.
- А TPU выполняет свои операции независимо от CPU.
- TensorFlow на CPU должен работать всё время, пока выполняется программа, использующая TPU (Это связано с тем, что CPU
  задачи управления, которые необходимы для работы TPU)
- Данные (например, изображения или тексты) загружаются в память CPU и подготавливаются (например, нормализуются и
  батчируются).
- Подготовленные данные передаются из памяти CPU в память TPU. Это происходит через высокоскоростной канал (PCIe)
- TPU выполняет нужные операции на переданных данных.
- Результаты вычислений сохраняются в памяти TPU.
- Результаты передаются обратно в память CPU для дальнейшей обработки
- После завершения программы TensorFlow на CPU освобождает ресурсы TPU и завершает работу.

## Какие компоненты нужны для работы TPU?

Чтобы TPU работал, нужна система:

1. Аппаратное обеспечение
   - TPU чип (специализированный ускоритель).
   - CPU (управляет TPU, загружает данные).
   - ОЗУ (RAM) (буфер для обработки данных).
   - SSD / HDD (хранение обученной модели).

2. Программное обеспечение

   Для работы с TPU нужен специальный софт.

   - Операционная система (Linux, macOS, Windows) — запускает код.
   - Фреймворки для AI:
      - TensorFlow XLA (компилятор моделей для TPU).
      - PyTorch XLA (аналог для PyTorch).
   - Драйвер TPU (соединяет TPU с CPU). Google не предоставляет драйверы для локального использования TPU, так как они
     оптимизированы для работы в облаке. (драйверы уже предустановлены в средах, где доступны TPU (например, Google
     Cloud или Google Colab) Драйверы для TPU не поставляются вместе с TensorFlow или XLA. Они предоставляются Google и
     доступны только в определенных средах:
   - Компилятор TPU (преобразует нейросетевые операции в инструкции TPU).

   Важно: Вся логика программы остаётся на CPU, а TPU делает только ускоренные вычисления.

## Взаимодействие с TPU

Через какие интерфейсы общается CPU и TPU?

Взаимодействие между CPU и TPU происходит через специализированные интерфейсы и шины данных, обеспечивающие передачу
команд, данных и результатов вычислений.

### Физические интерфейсы (Аппаратное взаимодействие)

CPU и TPU могут быть связаны разными способами, в зависимости от архитектуры TPU:

1. PCIe (PCI Express) – стандартный интерфейс для TPU-карт
    - Используется в TPU-картах.
   - Позволяет подключать TPU как ускоритель к CPU, аналогично видеокартам (GPU).
   - Обеспечивает высокую скорость передачи данных между CPU и TPU (от PCIe 3.0 до 5.0).

   Плюсы: Высокая скорость передачи, универсальность.

   Минусы: Ограниченная пропускная способность для больших тензоров.

2. NVLink (альтернатива PCIe, используется в суперкомпьютерах)
   - Разработан NVIDIA для связи GPU, но TPU тоже может поддерживать аналоги NVLink.
   - Прямое соединение TPU-TPU без посредничества CPU.
   - Намного быстрее, чем PCIe (до 900 ГБ/с).

   Плюсы: Высокая скорость соединения между TPU.

   Минусы: Требует специального оборудования.

3. Interconnect (Google TPU Pod – объединение TPU в кластер)
   - В облачных решениях Google TPU соединяются в TPU Pod через оптоволоконные или шины высокой пропускной способности.
   - Позволяет объединять сотни TPU в единую систему.

   Плюсы: Масштабируемость до суперкомпьютерных мощностей.

   Минусы: Используется только в облаке Google.

### Программное взаимодействие (Софт и API для связи CPU и TPU)

Физическое соединение (PCIe, HBM) — это только транспорт. Чтобы CPU мог передавать команды TPU, используются драйверы и
API.

1. Драйвер TPU (TensorFlow XLA, PyTorch XLA)
   - Специальный драйвер, который преобразует команды CPU в инструкции для TPU.
   - Оптимизирует выполнение тензорных операций.
   - Используется в TensorFlow XLA (Accelerated Linear Algebra) и PyTorch XLA.

2. API для программирования TPU.
   Для работы с TPU используются специальные API, например:
   - TensorFlow TPU API
   - PyTorch XLA
   - Google Cloud TPU API

   Эти API автоматически направляют вычисления на TPU, оптимизируя код.

Как выглядит процесс взаимодействия CPU и TPU?

---

# Архитектура процессора TPU

## Архитектура

TPU не попадает в классические архитектуры CISC или RISC, так как:

- Он не универсальный процессор (как CPU), а специализированный чип.
- TPU не выполняет стандартные команды общего назначения (арифметика, ветвления).
- Он использует векторно-матричные команды, управляемые XLA-компилятором.

Если пытаться классифицировать:

- Не CISC, потому что нет сложных инструкций вроде MOV, ADD, JMP.
- Не классический RISC, так как TPU не выполняет независимые команды, а работает с обработкой тензоров.
- Ближе всего TPU к VLIW (Very Long Instruction Word), где за один такт выполняются несколько операций параллельно.

## Разрядность

Внутренняя архитектура — 32-битная, но оптимизирована для работы с 8/16-битными тензорами.
Выбор 32-битной архитектуры объясняется несколькими причинами:

1. Машинное обучение не требует 64-битной точности
    - Для AI достаточно 16 или 8, так как нейросети устойчивы к небольшим численным ошибкам.
    - 64-битные вычисления увеличили бы энергопотребление и снизили бы эффективность TPU без значительного прироста
      качества обучения.

2. Оптимизация памяти и энергопотребления
    - Чем больше бит занимает число, тем больше памяти требуется для его хранения.
    - 64-битные числа занимали бы вдвое больше памяти, увеличивая потребность в пропускной способности и
      энергопотребление.
    - Использование 16 бит позволяет загружать в 4 раза больше данных в ту же память по сравнению с 64.

3. Улучшенная пропускная способность
    - TPU использует HBM (High Bandwidth Memory), но её пропускная способность ограничена.
    - При 64-битных данных пропускная способность упала бы в 2 раза, а для нейросетей важно питать MXU данными без
      задержек.
    - Использование 16- и 8-битных форматов позволяет выполнять в 4-8 раз больше операций за один такт.

4. MXU (Matrix Multiply Unit) работает с низкой разрядностью
    - Главный вычислительный блок TPU (MXU) спроектирован специально для 8/16 бит.
    - Включение 64-битных операций потребовало бы изменения всей архитектуры и значительно снизило бы эффективность.

## Фон-Неймановская vs Гарвардская архитектура

Отказ от традиционной модели памяти (фон-неймановской архитектуры)

### Проблема CPU и GPU: память – "узкое место".

В традиционной фон-неймановской архитектуре данные хранятся в оперативной памяти и передаются процессору по общей
шине данных.
Шина данных становится узким местом, поскольку CPU/GPU ждут загрузки данных вместо выполнения вычислений.
Даже у GPU, использующего высокоскоростную VRAM, всё ещё есть задержки на передачу данных.

### Решение TPU: гарвардская архитектура с локальной памятью.

TPU использует специализированную архитектуру, в которой данные загружаются заранее и хранятся внутри самого процессора.
Нет необходимости каждый раз загружать данные из RAM, что убирает задержки и увеличивает пропускную способность.
TPU использует массивно-параллельные вычисления, где все операции выполняются одновременно, без ожидания данных из
памяти.

### Что это даёт?

- Меньше задержек – TPU не ждёт, пока данные загрузятся из RAM.
- Более высокая пропускная способность – тензоры хранятся внутри TPU, обеспечивая мгновенный доступ.
- Минимальные накладные расходы – нет постоянного обмена данными, как в CPU и GPU.

CPU и GPU: Используют фон-неймановскую архитектуру, где память и процессор разделены.
Данные и инструкции хранятся в разных местах, передача идёт через общую шину, что создаёт задержки.

TPU: Использует гарвардскую архитектуру, где память и вычислительные блоки близко расположены или объединены.
Данные не загружаются из внешней RAM, а хранятся внутри TPU, что уменьшает задержки и увеличивает скорость работы.

## Состав TPU

Ключевые технологии TPU:

- MXU (Matrix Multiply Unit) – сердце TPU
    - MXU – это специализированный блок для умножения матриц.
    - Это основной вычислительный блок TPU, оптимизированный для выполнения операций умножения матриц.
    - MXU может выполнять тысячи операций умножения и сложения за один такт.

- Встроенная память HBM (High Bandwidth Memory)
    - Это высокоскоростная память, которая используется для хранения данных и промежуточных результатов вычислений.
    - HBM обеспечивает высокую пропускную способность, что позволяет быстро передавать данные между памятью и
      вычислительными блоками.
    - Данные загружаются один раз и остаются в TPU.
    - Нет узкого места памяти, как у CPU и GPU.

- Unified Buffer (UB):
    - Это быстрая память, которая используется для хранения промежуточных данных и результатов вычислений.
    - Unified Buffer меньше по объему, чем HBM, но быстрее и используется для временного хранения данных.

- Activation Unit:
    - Этот блок отвечает за выполнение нелинейных операций, таких как функции активации (например, ReLU, sigmoid).
    - Activation Unit работает совместно с MXU для выполнения полного цикла вычислений.

- Accumulators:
    - Это регистры, которые используются для накопления промежуточных результатов вычислений.
    - Accumulators позволяют выполнять операции, такие как суммирование или усреднение, без необходимости сохранять
      промежуточные результаты в памяти.

- Control Unit:
    - Это блок управления, который координирует работу всех компонентов TPU.
    - Control Unit управляет выполнением инструкций, передачей данных и синхронизацией между блоками.

## Как работают компоненты TPU?

Загрузка данных:
Данные загружаются из HBM в Unified Buffer.
Unified Buffer используется для временного хранения данных, которые будут использоваться в вычислениях.

Выполнение операций:
Данные передаются из Unified Buffer в MXU.
MXU выполняет операции умножения матриц и сложения.
Промежуточные результаты сохраняются в Accumulators.

Нелинейные операции:
Промежуточные результаты передаются в Activation Unit для выполнения нелинейных операций (например, ReLU).
Результаты возвращаются в Unified Buffer или HBM.

Сохранение результатов:
Окончательные результаты вычислений сохраняются в HBM.
Результаты могут быть переданы обратно в CPU или использованы для следующих вычислений.

### Итог

- TPU состоит из нескольких ключевых компонентов, таких как MXU, HBM, Unified Buffer, Activation Unit, Accumulators,
  Control Unit и Interconnect.
- MXU — это основной вычислительный блок, оптимизированный для выполнения операций умножения матриц.
- HBM — это высокоскоростная память, которая обеспечивает высокую пропускную способность.
- Unified Buffer — это быстрая память, которая используется для временного хранения данных.
- Activation Unit отвечает за выполнение нелинейных операций.
- Accumulators используются для накопления промежуточных результатов вычислений.
- Control Unit координирует работу всех компонентов TPU.
- Interconnect соединяет различные компоненты TPU и позволяет им обмениваться данными.

## Обработка данных: последовательность vs параллельность

| Характеристика	                    | CPU	              | GPU	               | TPU                     |
|------------------------------------|-------------------|--------------------|-------------------------|
| Основной тип вычислений	           | Последовательные	 | Параллельные	      | Массово параллельные    |
| Количество ядер	                   | 2-64 мощных ядра	 | 1000+ слабых ядер	 | Массив тензорных блоков |
| Оптимизация для матричных операций | 	  ❌	             | ⚠️ Частично	       | ✅ Полностью             |

CPU: Ориентирован на последовательную обработку, его ядра выполняют разные задачи, но работают по очереди.
Даже у многопоточных CPU узким местом остаётся ограничение по количеству потоков и пропускной способности памяти.

GPU: Ориентирован на массовую параллельность, имеет сотни или тысячи потоков, которые выполняют одни и те же инструкции
над разными данными (SIMD).
Подходит для графики и нейросетей, но не идеально оптимизирован для матриц.

TPU: Использует специальные тензорные процессоры (MXU) для максимально параллельных матричных операций.
Не просто параллельные вычисления, а массовая параллельность с минимумом накладных операций.

## Использование памяти и шины

| Характеристика	 		        | CPU                        | GPU                                  | TPU                  |
|---------------------------|----------------------------|--------------------------------------|----------------------|
| Где хранятся данные?			   | В оперативной памяти (RAM) | В видеопамяти (VRAM)                 | Внутри TPU           |
| Ограничение памяти			     | Узкое место: шина          | VRAM быстрее, но всё ещё ограничение | Минимальные задержки |
| Пропускная способность			 | Низкая                     | Средняя                              | Очень высокая        |

CPU и GPU:

Часто простаивают, ожидая загрузку данных из памяти.
GPU быстрее CPU, так как использует видеопамять (VRAM), но всё равно зависит от пропускной способности памяти.

TPU:

Данные уже загружены в TPU, нет нужды в постоянном обмене с RAM.
Реализован ускоренный доступ к данным внутри процессора.
Резко снижены накладные расходы на перемещение данных.

## Вычислительные блоки (ALU, CUDA, MXU)

| Блок			                                  | CPU | GPU | TPU |
|------------------------------------------|-----|-----|-----|
| Арифметико-логическое устройство (ALU)		 | ✅   | ✅   | ❌   |
| CUDA/Shader Execution Units	             | ❌   | ✅   | ❌   |
| Тензорные процессоры (MXU)	              | ❌   | ❌   | ✅   |

CPU: Использует ALU (Arithmetic Logic Unit) для выполнения инструкций.
Подходит для разноплановых вычислений, но не оптимизирован для нейросетей.

GPU: Использует CUDA-ядра (у NVIDIA) или Shader Execution Units (у AMD).
Хорош для параллельных операций, но не всегда эффективен для нейросетей.

TPU: Основной вычислительный блок – MXU (Matrix Multiply Unit).
MXU обрабатывает матрицы напрямую, обеспечивая гигантскую пропускную способность для AI.
Не нужны сложные инструкции (как у CPU/GPU), всё сосредоточено на быстром умножении матриц.

## Энергопотребление и эффективность

| Характеристика			        | CPU               | GPU                      | TPU              |
|--------------------------|-------------------|--------------------------|------------------|
| Энергопотребление			     | Высокое 50-150 Вт | Очень высокое 200-500 Вт | Низкое 30-100 Вт |
| Эффективность AI-задач		 | Плохая            | Средняя                  | Высокая          |
| Тепловыделение           | Высокое           | Очень высокое            | Низкое           |

CPU: Потребляет много энергии на управление потоками, памятью и инструкциями.
Мало полезных вычислений на ватт потребляемой мощности.

GPU: Высокая мощность, но много "лишних" вычислений.
Эффективнее CPU, но всё ещё много накладных расходов.

TPU: Оптимизирован под AI, не тратит энергию на ненужные инструкции.
Минимальные накладные расходы = больше вычислений за ту же мощность.

Высокая производительность не всегда означает большие затраты энергии. TPU не только быстрее, но и намного эффективнее
по энергопотреблению.

Почему TPU потребляет меньше энергии?

- Нет сложной логики управления, как у CPU.
- Нет больших накладных расходов на память, как у GPU.
- Оптимизирован для AI → выполняет только нужные операции.



# ВКЛЮЧИТЬ В ТЕКСТ:

- почему в AI требуются вычисления матриц (тензорные вычисления)
